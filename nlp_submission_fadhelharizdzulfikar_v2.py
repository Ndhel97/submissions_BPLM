# -*- coding: utf-8 -*-
"""nlp-submission-fadhelharizdzulfikar_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wGOFvKiAkdLhV1KG9eMaqBujDrbT4c23
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_json('/content/drive/My Drive/Dicoding/Belajar Pengembangan Machine Learning/News_Category_Dataset_v2.json', lines=True)


df.head()

df.shape

len(df.category.unique())

"""There are 41 categories in this data. And i think it is too many for me as a beginner. So, I only take 5 categories for this submission. (POLITICS, FOOD & DRINK, BUSINESS, HOME & LIVING, TRAVEL)"""

df_first = df.loc[(df['category'] == 'POLITICS') | (df['category'] == 'FOOD & DRINK') | (df['category'] == 'BUSINESS') | (df['category'] == 'HOME & LIVING') | (df['category'] == 'TRAVEL')]

"""Delete some columns"""

df_second = df_first.drop(columns=['authors', 'link', 'date'])

"""Dataframe dimension"""

df_second.shape

"""Now, the data only contains 58984 rows.

Import libraries
"""

from keras.layers import Input, LSTM, Bidirectional, SpatialDropout1D, Dropout, Flatten, Dense, Embedding, BatchNormalization
from keras.models import Model
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import nltk, os, re, string
from nltk.corpus import stopwords

# nltk.download() #wordnet stopwords

"""Clean the data"""

# LOWER CASE ALL CHARACTERS 
df_second.headline = df_second.headline.apply(lambda x: x.lower())
df_second.short_description = df_second.short_description.apply(lambda x: x.lower())

## LEMMATIZATION
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
lemmatizer = WordNetLemmatizer()

def lem(text):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(text.split())]))

df_second.headline = df_second.headline.apply(lambda x: lem(x))
df_second.short_description = df_second.short_description.apply(lambda x: lem(x))

# REMOVING PUNCTUATION
def cleaner(text):
    return(text.translate(str.maketrans('','', string.punctuation)))
df_second.headline = df_second.headline.apply(lambda x: cleaner(x))
df_second.short_description = df_second.short_description.apply(lambda x: lem(x))

# REMOVING NUMBERS
def rem_numbers(text):
    return re.sub('[0-9]+','',text)
df_second['headline'].apply(rem_numbers)
df_second['short_description'].apply(rem_numbers)

# REMOVING STOPWORDS
st_words = stopwords.words()
def stopword(text):
    return(' '.join([w for w in text.split() if w not in st_words ]))
df_second.headline = df_second.headline.apply(lambda x: stopword(x))
df_second.short_description = df_second.short_description.apply(lambda x: lem(x))

df_second.head()

"""Apply one hot encoding to the data."""

category = pd.get_dummies(df_second.category)
df_third = pd.concat([df_second, category], axis=1)
df_third = df_third.drop(columns='category')
df_third

"""concantenate headlines column with each short_description into headline dataframe. Put the labels in label dataframe."""

headline = df_third['short_description'].values + ' ' + df_third['headline'].values
label = df_third[['FOOD & DRINK', 'HOME & LIVING', 'POLITICS', 'BUSINESS', 'TRAVEL']].values

headline

"""Split data into training and validation"""

from sklearn.model_selection import train_test_split
headline_train, headline_test, label_train, label_test = train_test_split(headline, label, test_size=0.2, shuffle=True)

"""Process each words into token using tokenize function from Keras."""

max_len = 256

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(lower=True, char_level=False)
tokenizer.fit_on_texts(headline_train) 
tokenizer.fit_on_texts(headline_test)
	 
sekuens_train = tokenizer.texts_to_sequences(headline_train)
sekuens_test = tokenizer.texts_to_sequences(headline_test)
	 
padded_train = pad_sequences(sekuens_train, padding='post', maxlen=max_len) 
padded_test = pad_sequences(sekuens_test, padding='post', maxlen=max_len)

word_to_index = tokenizer.word_index

"""Use GloVe learning algorithm for obtaining vector representations for words devolped by Stanford"""

vocab_size =  len(word_to_index)
oov_tok = "<OOV>"
embedding_dim = 200

import numpy as np
embeddings_index = {};

# !unzip '/content/drive/My Drive/Dicoding/Belajar Pengembangan Machine Learning/1835_3176_compressed_glove.6B.200d.txt.zip'

with open('/content/glove.6B.200d.txt') as f:
    for line in f:
        values = line.split();
        word = values[0];
        coefs = np.asarray(values[1:], dtype='float32');
        embeddings_index[word] = coefs;

embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));
for word, i in word_to_index.items():
    embedding_vector = embeddings_index.get(word);
    if embedding_vector is not None:
        embeddings_matrix[i] = embedding_vector;

"""Design the model."""

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, weights=[embeddings_matrix], trainable=False, input_length = max_len),
    tf.keras.layers.Bidirectional(LSTM(256, return_sequences=True)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.SpatialDropout1D(0.5),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128,activation = 'relu'),
    tf.keras.layers.Dense(64,activation = 'relu'),
    tf.keras.layers.Dense(32,activation = 'relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])

# model = modeling()
model.compile(optimizer = 'adam', metrics = ['accuracy'], loss= 'categorical_crossentropy')
model.summary()

"""Train the model."""

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', patience=5)

num_epochs = 20
history = model.fit(padded_train, label_train, epochs=num_epochs, 
	                    validation_data=(padded_test, label_test), verbose=2, callbacks=[es])

"""As you can see, we got validation accuracy about 90%. Next, plot the accuracy and loss."""

print(history.history.keys())

import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

